# 科大讯飞
### 1.LLM训练过程的超参数怎么设置的？
    微调方式采用lora
    lora的r设置为8
    学习率1e-5
    batch需要根据显存占用调整，一般为2的N次方，可以用梯度累积来增加batch-size
    epoch一般设置为6，在训练过程中观察验证集的损失变化情况，进行早停处理
    精度采用FP16
    显存不充足时候 gradient-checkpoint设置为 True
    warm-up-step 酌情设置
    是否使用deepspeed，以及stage-几，和cpu-offload,根据显存设置
### 2.怎么预估LLM训练的batch-size?
    题外话，模型训练数据和参数大小的关系是：训练一个6B模型需要 20X6B的token数
    以一个GLM-6B为例，FP16全参数微调，不考虑其他显存优化需要的显存为：
    模型参数：6 * 2 = 12G
    梯度： 6 * 2 = 12G
    优化器状态： 6 * 12 =72G
    中间激活值：batch=1，len=2048, 需要约27G，batch或者len翻倍，显存翻倍

### 3.LLM-SFT的数据来源，数据量，训练过程你是怎么迭代的？
    初始数据通过gpt4 few-shot方式获取，人工选取其中合格的样本，训练出第一版模型
    用第一版模型的预测结果，人工修改作为接下来的迭代数据。
    数据总量3k，试验证明数据量在3k以后，单纯增加数据量提升不明显，提高数据质量有效。

### 4.逻辑回归是线性的么？
    不是，逻辑回归是线性回归+sigmoid，线性回归是线性的，但是sigmoid带来了非线性的因子。尽管它号称广义线性模型
### 5.bert模型做分类，你是怎么使用的？
    bert的输出有last_hidden_state, pool_output
    last_hidden_state对应cls和每个token的输出[batch, seq_len, 768]，pool_output是cls接一个全连接层 + tanh激活层的输出[batch, 768]
    如果分类任务，一般采用pool_output直接加一个线性层
    序列标注任务，取last_hidden_state每个token的输出
### 6.bert有什么缺点/局限性
    1.预训练时候的【mask】在下游任务微调时候并不出现，这就造成了一定的信息偏差，mask的811策略（80%mask，10%随机替换，10%不变）只能缓解
    2.预训练的MLM任务，每个batch只有15%的token参与训练，收敛速度慢
### 7.bert做分类时候，如果遇到过拟合怎么办？
    1.观察验证集损失信息，早停
    2.增加训练数据
    3.dropout
    4.选用小点的模型，减少参数量
    5.数据清洗
### 8.bert数据不均衡怎么处理？其他模型怎么处理
    1.数据层面，增加量少类别的样本（转译，同义词替换，LLM做相同语义文本生成），量多类别样本欠采样。
    2.模型层面，损失函数可以改为带权重的交叉熵损失
    3.评估指标，关注较少类别的召回率（所有该类别的样本，被正确找出来的比例）
### 9.负采样怎么做？可以用fasttext或word2vec的负采样举例
    负采样是一种用于词嵌入模型训练的采样方式，如果使用softmax进行全词表的计算，计算量很大。因此只使用正样本和采样的几个负样本计算损失，更新他们的权重。
    采样方式：随机或者按词频分布以一定概率采样
### 10.你的强化学习DPO是怎么训练的？为什么不用PPO？
    1.数据，一个input对应一个chosen和一个reject；来源是推理产生的Badcase作为reject，人工修改后的座位chosen；数据量5k；
    2.模型,策略模型Policy和参照模型Reference,Policy参与训练，Reference参数固定
    3.损失，Policy输出chosen和reject的logits_prob之差作为Policy得分,Ref输出chosen和reject的logits_prob之差作为Ref得分,loss = -logsigmoid(系数β * （Pilicy得分-Ref得分）
    4.参数，epoch=1，学习率1e-5, β=0.1

    PPO需要用到4个模型，显存开销大于DPO，以训练过程不稳定和效果不稳定著称；
    PPO的Reward-model难训练，而DPO也能起效果，从结果论上选择DPO

### 11.GLM,Baichuan,Qwen,LLama这些模型哪个更好用，你怎么评估哪个更好用的，他们的结构说一下。
    开源时间从开始的LLama到GLM，再到Baichuan，Qwen。后出来的会比先出来的好用，目前感觉第一梯队是GLM4，Qwen

    评估模型：
        1.针对专利和期刊中的高频技术词进行提问，判断模型训练数据是否包含该知识点
        2.采用prompt或few-shot方式，引导模型按指令要求输出，判断模型对指令的遵循能力
        3.同样数据sft之后的模型，人工对其输出进行评估

    模型结构：
        相同点：
        结构基本都是Decoder-Only，在transformer-decoder的基础上删减中间的Block。
        Norm位置：Post-Norm改为Pre-Norm，Pre-Nrom在残差计算之前，由于不是所有参数都正则化，更难出现梯度消失，训练更稳定。
        Nrom计算方式：RMSNorm，普通的Norm是每个元素减平局值，再除标准差。RMSNorm在分子和分母部分，都不再减去平均值
        Deep-Norm:在计算Norm前会扩大残差连接（乘α）；降低了初始化参数的比例
        激活函数：SwiGlue, x·(sigmoid(βx))， 负值时有梯度，0点可导（这些glue也有，为什么更优无法解释）
        位置编码：外推基本采用ROPE， Baichuan-13B是Alibi
        
        不同点：
        GLM的FFN层只有 线性层--激活层--线性层--dropout， 而其他模型则是 （（线性层-激活层）x 线性层）-线性层
        GLM1的Mask是input部分双向，output单向，在GLM2阶段改成和其他模型一样只有单向
    
### 12.Batch-Norm和Layer-Norm的区别，为什么CV用BN，NLP用LN？
    从概念上来说：
    两者计算均值和标准差的维度不一样，以文本的[batch, seq-len, emb-dim]和图像的NCHW为例
    BN-NLP是一个batch中相同位置的token做Norm，BN-CV是在batch方向做Norm（取所有图的通道1在一起计算），维度是NHW
    LN-NLP是seq-len和emb-dim做Norm（计算一条文本的Norm），LN-CV是在通道CV方向做Norm（每张图3个通道一起计算），维度是CHW
    IN-NLP是每个token在emb-dim做Norm，IN-CV是对HW做Norm。
    Note:NLP-LN实际进行的是IN操作

    BN需要较大的batch-size,但是一个batch的文本长度不一致，越往后padding 0就越多，计算过程充斥大量的0，所以BN不适合NLP的变长。

### 13.常见损失函数
    分类：
        0-1损失，hinge损失，cross-entropy
    回归：
        绝对值损失，平方损失 
