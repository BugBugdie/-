### 1.Vllm
    Vllm是一个大模型推理框架，两个主要特征，
    continue-batch：一个batch推理时，生成的结果有长有短，短的结束后需要等待长的结束再一起返回，CB的方式是结束后马上补充下一个句子，高效利用显存。
    page-attention：推理过程中的长kv-cache需要一片连续的存储空间，vllm把显存物理空间分成一个个小块，使用连续的逻辑空间映射不连续的物理空间，提高显存利用率。对于同一prompt生成多个结果的情况，vllm多个逻辑空间映射到同一物理空间
